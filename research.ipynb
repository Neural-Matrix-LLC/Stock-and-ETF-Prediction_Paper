{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59bd390b-21ca-4ae5-b3bb-4a31dc903eb9",
   "metadata": {},
   "source": [
    "# Stock Price Prediction\n",
    "\n",
    "**Keywords:** Stock price prediciton, Deep Learning, RNN, LSTM, Bi-LSTM, Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e108c21a-c35b-4ccd-b805-429c162702c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Recurrent Neural Network (RNN)\n",
    "\n",
    "A recurrent neural network (RNN) is a type of artificial neural network where the computation graph contains directed cycles. \n",
    "\n",
    "Since a RNN's hidden layers have connections back to themselves, the states of the hidden layers at one time instant to be used as input to the hidden layers at the next time instant. This allows hidden states to capture information about the temporal relation between input sequences and output sequences.\n",
    "\n",
    "In simplest terms, the following equations define how an RNN evolves over time:\n",
    "$$ o^t = f(h^t; \\theta) $$\n",
    "$$ h^t = g(h^{t-1}, x^t; \\theta) $$\n",
    "where where $o^t$ is the output of the RNN at time $t$, $x^t$ is the input to the RNN at time $t$, and $h^t$ is the state of the hidden layer(s) at time $t$.\n",
    "\n",
    "The image below outlines a simple graphical model to illustrate the relation between these three variables in an RNN's computation graph.\n",
    "\n",
    "<center><img src=\"img/rnn.png\" width=\"150px\"></center>\n",
    "\n",
    "*A graphical model for an RNN. The values $\\theta_i$, $\\theta_h$, and $\\theta_o$ represent the parameters associated with the inputs, previous hidden layer states, and outputs, respectively*\n",
    "\n",
    "The first equation says that, given parameters $\\theta$ (which encapsulates the weights and biases for the network), the output at time $t$ depends only on the state of the hidden layer at time $t$. \n",
    "\n",
    "The second equation says that, given the same parameters $\\theta$, the hidden layer at time $t$ depends on the hidden layer at time $t-1$ and the input at time $t$. This second equation demonstrates that the RNN can remember its past by allowing past computations $h^{t-1}$ to influence the present computations $h^{t}$.\n",
    "\n",
    "One issue with RNNs in general is known as the vanishing/exploding gradients problem. This problem states that, for long input-output sequences, RNNs have trouble modeling long-term dependencies, that is, the relationship between elements in the sequence that are separated by large periods of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7268a2a-3ff8-4d9c-96d3-b630d5952497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acb11b32-abc4-4651-813a-07af63caa8a2",
   "metadata": {},
   "source": [
    "## Work Cited\n",
    "\n",
    "Patel, Janik, et al. \"Stock Price Prediction Using RNN and LSTM.\" *JETIR*, vol.5, no. 11, Nov 18. 2018. \n",
    "https://www.jetir.org/papers/JETIRK006164.pdf\n",
    "\n",
    "McGonagle, John, et al. \"Recurrent Neural Network.\" *Brilliant.org.* https://brilliant.org/wiki/recurrent-neural-network/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b6f10-b34f-4691-bd2b-a144cbb47fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
